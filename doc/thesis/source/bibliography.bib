@article{Krishnaveni2017,
	author = {Krishnaveni and Prabakaran},
	file = {:D$\backslash$:/Daten/Downloads/59.pdf:pdf},
	keywords = {attack patterns,cloud computing,combinatorial testing,security testing,xss cross site scripting},
	number = {6},
	pages = {437--445},
	title = {{Security testing and comparing vulnerability detection tools for cloud based saas applications}},
	volume = {115},
	year = {2017}
}
@article{Shahriar2009,
	abstract = {Cross Site Scripting (XSS) is one of the worst vulnerabilities that allow malicious attacks such as cookie thefts and Web page defacements. Testing an implementation against XSS vulnerabilities (XSSVs) can avoid these consequences. Obtaining an adequate test data set is essential for testing of XSSVs. An adequate test data set contains effective test cases that can reveal XSSVs. Unfortunately, traditional testing techniques for XSSVs do not address the issue of adequate testing. In this work, we apply the idea of mutation-based testing technique to generate adequate test data sets for testing XSSVs. Our work addresses XSSVs related to Web-applications that use PHP and JavaScript code to generate dynamic HTML contents. We propose 11 mutation operators to force the generation of adequate test data set. A prototype mutation-based testing tool named MUTEC is developed to generate mutants automatically. The proposed operators are validated by using five open source applications having XSSVs. The results indicate that the proposed operators are effective for testing XSSVs.},
	author = {Shahriar, Hossain and Zulkernine, Mohammad},
	doi = {10.1109/IWSESS.2009.5068458},
	file = {:D$\backslash$:/Daten/Downloads/2009-ICSE-SESS-XSS-Shahriar.pdf:pdf},
	isbn = {9781424437252},
	issn = {00206598},
	journal = {Proceedings of the 2009 ICSE Workshop on Software Engineering for Secure Systems, SESS 2009},
	number = {May 2009},
	pages = {47--53},
	title = {{MUTEC: Mutation-based testing of cross site scripting}},
	year = {2009}
}

@misc{Kurtishaj2016,
	author = {html5sec},
	keywords = {xss,filter evasion,waf,polyglots},
	mendeley-tags = {xss,filter evasion,waf,polyglots},
	title = {{XSS Polyglots - The Context Contest}},
	url = {https://blog.bugcrowd.com/xss-polyglots-the-context-contest},
	note = { Zugriff: 2017-11-07},
	year = {}
}
@misc{HTML5Sec2017,
	author = {html5sec},
	keywords = {filter evasion,html5,css3,waf,xss},
	mendeley-tags = {xss,filter evasion,waf,owasp},
	title = {{HTML5 Security Cheat Sheet}},
	url = {https://html5sec.org/},
	note = { Zugriff: 2017-10-27},
	year = {}
}
@misc{OWASP2012,
author = {OWASP},
keywords = {filter evasion,owasp,waf,xss},
mendeley-tags = {xss,filter evasion,waf,owasp},
title = {{Filter Evasion Cheat Sheet}},
url = {https://www.owasp.org/index.php/XSS\_Filter\_Evasion\_Cheat\_Sheet},
note = { Zugriff: 2017-10-17},
year = {2012}
}
@misc{Group2014,
author = {Group, W3C Working},
title = {{HTML5 Differences from HTML4}},
url = {https://www.w3.org/TR/2014/NOTE-html5-diff-20141209/},
note = { Zugriff: 2017-10-17},
year = {2014}
}
@misc{Wichers2017,
author = {Wichers, Dave},
keywords = {OWASP,XSS,types},
mendeley-tags = {OWASP,XSS,types},
title = {{Types of Cross-Site Scripting}},
url = {https://www.owasp.org/index.php/Types\_of\_Cross-Site\_Scripting},
note = { Zugriff: 2017-10-17},
year = {2017}
}
@misc{Seals2016,
author = {Seals, Tara},
keywords = {open source,sql injections,web,xss},
mendeley-tags = {open source,sql injections,web,xss},
title = {{87{\%} of Open-Source Vulns Are XSS and SQL Injection}},
url = {https://www.infosecurity-magazine.com/news/87-of-opensource-vulns-are-xss-and/},
note = { Zugriff: 2017-10-17},
year = {2016}
}
@misc{Norse2017,
author = {Norse},
keywords = {attack,http,penetration},
mendeley-tags = {attack,http,penetration},
title = {{Norse}},
url = {http://map.norsecorp.com/\#/?protocol=http},
note = { Zugriff: 2017-10-17},
year = {2017}
}
@misc{Lackes,
author = {Lackes, Prof. Dr. Richard},
keywords = {Internet,Web2.0,definition},
mendeley-tags = {Internet,Web2.0,definition},
title = {{Web2.0}},
url = {http://wirtschaftslexikon.gabler.de/Definition/web-2-0.html\#definition},
note = { Zugriff: 2017-10-17}
}
@article{WhiteHatSecurity2016,
author = {{White Hat Security}},
file = {:D$\backslash$:/Daten/Downloads/WH-2016-Stats-Report-FINAL.pdf:pdf},
pages = {44},
title = {{Web applications security statistics report 2016}},
url = {https://info.whitehatsec.com/rs/675-YBI-674/images/WH-2016-Stats-Report-FINAL.pdf},
year = {2016}
}
@misc{Pollet-Villard2016,
author = {Pollet-Villard, Sylvain},
keywords = {6chars,esoteric,js,nonalpha},
mendeley-tags = {6chars,esoteric,js,nonalpha},
title = {{6CharsJS}},
url = {https://syllab.fr/projets/experiments/sixcharsjs/ http://slides.com/sylvainpv/xchars-js\#/32},
note = { Zugriff: 2017-09-15},
year = {2016}
}
@misc{Kleppe,
abstract = {JSFuck is an esoteric and educational programming style based on the atomic parts of JavaScript. It uses only six different characters to write and execute code.},
author = {Kleppe, Martin},
keywords = {JavaScript,esoteric},
mendeley-tags = {JavaScript,esoteric},
title = {{JSFuck}},
url = {http://www.jsfuck.com/},
note = { Zugriff: 2017-09-15},
year = {2012}
}
@inproceedings{Duchene2013a,
abstract = {We present an approach to detect web injection vulnerabilities by generating test inputs using a combination of model inference and evolutionary fuzzing. Model inference is used to obtain a knowledge about the application behavior. Based on this understanding, inputs are generated using genetic algorithm (GA). GA uses the learned formal model to automatically generate inputs with better fitness values towards triggering an instance of the given vulnerability.},
annote = {From Duplicate 2 (XSS vulnerability detection using model inference assisted evolutionary fuzzing - Duchene, Fabien; Groz, Roland; Rawat, Sanjay; Richier, Jean Luc)

- evolutionary fuzzing
-- creating generations of inputs based on a attack grammer
-- calculating fitness of an input
-- Kameleon-Fuzz is a implementation of this approach},
author = {Duchene, Fabien and Groz, Roland and Rawat, Sanjay and Richier, Jean-luc Luc},
booktitle = {SECTEST 2012 - 3rd International Workshop on Security Testing (affiliated with ICST)},
doi = {10.1109/ICST.2012.181},
file = {:C$\backslash$:/Users/alex/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Duchene et al. - 2013 - XSS Vulnerability Detection Using Model Inference Assisted Evolutionary Fuzzing(2).pdf:pdf;:C$\backslash$:/Users/alex/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Duchene et al. - 2013 - XSS Vulnerability Detection Using Model Inference Assisted Evolutionary Fuzzing(3).pdf:pdf},
isbn = {9780769546704},
issn = {2159-4848},
keywords = {Black-Box Security Testing,Genetic Algorithm,Model Based Fuzzing,Model Inference,Test Automation},
number = {Itea 2},
pages = {815--817},
title = {{XSS Vulnerability Detection Using Model Inference Assisted Evolutionary Fuzzing}},
year = {2013}
}
@inproceedings{Tripp2013,
abstract = {Black-box security testing of web applications is a hard prob-lem. The main complication lies in the black-box assump-tion: The testing tool has limited insight into the workings of server-side defenses. This has traditionally led commer-cial as well as research vulnerability scanners toward heuris-tic approaches, such as testing each input point (e.g. HTTP parameter) with a short, predefined list of effective test pay-loads to balance between coverage and performance. We take a fresh approach to the problem of security test-ing, casting it into a learning setting. In our approach, the testing algorithm has available a comprehensive database of test payloads, such that if the web application's defenses are broken, then with near certainty one of the candidate pay-loads is able to demonstrate the vulnerability. The question then becomes how to efficiently search through the payload space to find a good candidate. In our solution, the learning algorithm infers from a failed test—by analyzing the web-site's response—which other payloads are also likely to fail, thereby pruning substantial portions of the search space. We have realized our approach in XSS Analyzer, an industry-level cross-site scripting (XSS) scanner featuring 500,000,000 test payloads. Our evaluation on 15,552 bench-marks shows solid results: XSS Analyzer achieves {\textgreater} 99{\%} coverage relative to brute-force traversal over all payloads, while trying only 10 payloads on average per input point. XSS Analyzer also outperforms several competing algo-rithms, including a mature commercial algorithm—featured in IBM Security AppScan Standard V8.5—by a far margin. XSS Analyzer has recently been integrated into the latest version of AppScan (V8.6) instead of that algorithm.},
author = {Tripp, Omer and Weisman, Omri and Guy, Lotem},
booktitle = {Proceedings of the 2013 International Symposium on Software Testing and Analysis},
doi = {10.1145/2483760.2483776},
file = {:C$\backslash$:/Users/alex/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tripp, Weisman, Guy - 2013 - Finding Your Way in the Testing Jungle A Learning Approach to Web Security Testing.pdf:pdf},
isbn = {978-1-4503-2159-4},
keywords = {cross-site scripting,online learning,vulnerability scanner},
pages = {347--357},
title = {{Finding Your Way in the Testing Jungle: A Learning Approach to Web Security Testing}},
url = {http://doi.acm.org/10.1145/2483760.2483776},
year = {2013}
}
@misc{Borgardt2017,
abstract = {A grammar extension to the open source project "dharma".},
author = {Borgardt, Alexander},
keywords = {definition,grammar,xss},
mendeley-tags = {definition,grammar,xss},
title = {{Dharma xss grammar}},
url = {https://github.com/MozillaSecurity/dharma/blob/master/dharma/grammars/xss.dg},
note = { Zugriff: 2017-09-08},
year = {2017}
}
@inproceedings{Godefroid2007,
abstract = {Fuzz testing is an effective technique for finding security vulnerabilities in software. Fuzz testing is a form of blackbox random testing which randomly mutates well-formed inputs and tests the program on the resulting data. In some cases, grammars are used to randomly generate the well-formed inputs. This also allows the tester to encode application-specific knowledge (such as corner cases of particular interest) as part of the grammar, and to specify test heuristics by assigning probabilistic weights to production rules. Although fuzz testing can be remarkably effective, the limitations of blackbox random testing are well-known. For instance, the then branch of the conditional statement "if (x==10) then" has only one in 232 chances of being exercised if x is a randomly chosen 32-bit input value. This intuitively explains why random testing usually provides low code coverage. Recently, we have proposed an alternative approach of whitebox fuzz testing 4, building upon recent advances in dynamic symbolic execution and test generation 2. Starting with a well-formed input, our approach symbolically executes the program dynamically and gathers constraints on inputs from conditional statements encountered along the way. The collected constraints are then systematically negated and solved with a constraint solver, yielding new inputs that exercise different execution paths in the program. This process is repeated using a novel search algorithm with a coverage-maximizing heuristic designed to find defects as fast as possible in large search spaces. For example, symbolic execution of the above code fragment on the input x = 0 generates the constraint x 10. Once this constraint is negated and solved, it yields x = 10, which gives us a new input that causes the program to follow the then branch of the given conditional statement. We have implemented this approach in SAGE (Scalable, Automated, Guided Execution), a tool based on x86 instruction-level tracing and emulation for whitebox fuzzing of file-reading Windows applications. While still in an early stage of development and deployment, SAGE has already discovered more than 30 new bugs in large shipped Windows applications including image processors, media players and file decoders. Several of these bugs are potentially exploitable memory access violations. In this talk, I will briefly review blackbox fuzzing for security testing. Then, I will present an overview of our recent work on whitebox fuzzing 4 (joint work with Michael Y. Levin and David Molnar), with an emphasis on the key algorithms and techniques needed to make this approach effective and scalable (see also 1, 3).},
author = {Godefroid, Patrice},
booktitle = {International Conference on Automated Software Engineering (ASE 2007)},
doi = {10.1145/1292414.1292416},
file = {:C$\backslash$:/Users/alex/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Godefroid - 2007 - Random testing for security blackbox vs. whitebox fuzzing.pdf:pdf;:D$\backslash$:/Daten/cloud.aborgardt.com/Documents/Studium/Masterstudium/[000] Masterarbeit/Materialien/talk-rt2007.pdf:pdf},
isbn = {9781595938817},
number = {November},
pages = {1},
title = {{Random testing for security: blackbox vs. whitebox fuzzing}},
year = {2007}
}
@article{Bozic2013,
abstract = {—Security issues of web applications are still a current topic of interest especially when considering the consequences of unintended behaviour. Such services might handle sensitive data about several thousands or millions of users. Hence, exploiting services or other undesired effects that cause harm on users has to be avoided. Therefore, for software developers of such applications one of the major tasks in providing security is to embed testing methodologies into the software development cycle, thus minimizing the subsequent damage resulting in debugging and time intensive upgrading. Model-based testing evolved as one of the methodologies which offer several theoretical and practical approaches in testing the system under test (SUT) that combine several input generation strategies like mutation testing, using of concrete and symbolic execution etc. by putting the emphasis on specification of the model of an application. In this work we propose an approach that makes use of an attack pattern model in form of a UML state machine for test case generation and execution. The paper also discusses the current implementation of our attack pattern testing tool using a XSS attack pattern and demonstrates the execution in a case study. Index Terms—Attack pattern model, cross-site scripting, model-based testing, security testing.},
annote = {From Duplicate 2 (XSS Pattern for Attack Modeling in Testing - Bozic, Josip; Wotawa, Franz)

- formal definition of an attack pattern for xss
- uml state machine
- xss type-1 {\&} type-2
- once implemented -{\textgreater} executes fully automatically and reports its status},
author = {Bozic, Josip and Wotawa, Franz},
file = {:C$\backslash$:/Users/alex/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bozic, Wotawa - 2013 - XSS Pattern for Attack Modeling in Testing(2).pdf:pdf},
isbn = {9781467361613},
keywords = {attack pattern model,cross-site scripting,model-based testing,security testing},
pages = {71--74},
title = {{XSS Pattern for Attack Modeling in Testing}},
year = {2013}
}
@article{Godefroid2008,
abstract = {Whitebox fuzzing is a form of automatic dynamic test generation, based on symbolic execution and constraint solving, designed for security testing of large applications. Unfortunately, the current effectiveness of whitebox fuzzing is limited when testing applications with highly-structured inputs, such as compilers and interpreters. These applications process their inputs in stages, such as lexing, parsing and evaluation. Due to the enormous number of control paths in early processing stages, whitebox fuzzing rarely reaches parts of the application beyond those first stages. In this paper, we study how to enhance whitebox fuzzing of complex structured-input applications with a grammar-based specification of their valid inputs. We present a novel dynamic test generation algorithm where symbolic execution directly generates grammar-based constraints whose satisfiability is checked using a custom grammar-based constraint solver. We have implemented this algorithm and evaluated it on a large security-critical application, the JavaScript interpreter of Internet Explorer 7 (IE7). Results of our experiments show that grammar-based whitebox fuzzing explores deeper program paths and avoids dead-ends due to non-parsable inputs. Compared to regular whitebox fuzzing, grammar-based whitebox fuzzing increased coverage of the code generation module of the IE7 JavaScript interpreter from 53{\%} to 81{\%} while using three times fewer tests.},
author = {Godefroid, Patrice and Kiezun, Adam and Levin, Michael Y.},
doi = {10.1145/1379022.1375607},
file = {:C$\backslash$:/Users/alex/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Godefroid, Kiezun, Levin - 2008 - Grammar-based whitebox fuzzing.pdf:pdf;:C$\backslash$:/Users/alex/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Godefroid, Kiezun, Levin - 2008 - Grammar-based whitebox fuzzing(2).pdf:pdf},
isbn = {9781595938602},
issn = {03621340},
journal = {ACM SIGPLAN Notices},
keywords = {all or part of,automatic test generation,grammars,is granted without fee,or hard copies of,permission to make digital,personal or classroom use,program verification,provided that copies are,software testing,this work for},
number = {6},
pages = {206},
title = {{Grammar-based whitebox fuzzing}},
volume = {43},
year = {2008}
}
@article{Doupe2010,
abstract = {Black-box web vulnerability scanners are a class of tools that can be used to identify security issues in web applications. These tools are often mar-keted as " point-and-click pentesting " tools that automatically evaluate the secu-rity of web applications with little or no human support. These tools access a web application in the same way users do, and, therefore, have the advantage of being independent of the particular technology used to implement the web application. However, these tools need to be able to access and test the application's various components, which are often hidden behind forms, JavaScript-generated links, and Flash applications. This paper presents an evaluation of eleven black-box web vulnerability scanners, both commercial and open-source. The evaluation composes different types of vulnerabilities with different challenges to the crawling capabilities of the tools. These tests are integrated in a realistic web application. The results of the evalu-ation show that crawling is a task that is as critical and challenging to the overall ability to detect vulnerabilities as the vulnerability detection techniques them-selves, and that many classes of vulnerabilities are completely overlooked by these tools, and thus research is required to improve the automated detection of these flaws.},
annote = {From Duplicate 1 (Why Johnny can't pentest: An analysis of black-box web vulnerability scanners - Doup{\'{e}}, Adam; Cova, Marco; Vigna, Giovanni)

- evaluating of web application crawler capabilities
- detecting entry points for xss attacks
- tested with wackopicko, a realistic full functional web application
- testet with 11 scanners},
author = {Doup{\'{e}}, Adam and Cova, Marco and Vigna, Giovanni},
doi = {10.1007/978-3-642-14215-4_7},
file = {:C$\backslash$:/Users/alex/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Doup{\'{e}}, Cova, Vigna - 2010 - Why Johnny can't pentest An analysis of black-box web vulnerability scanners(2).pdf:pdf},
isbn = {3642142141},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {111--131},
title = {{Why Johnny can't pentest: An analysis of black-box web vulnerability scanners}},
volume = {6201 LNCS},
year = {2010}
}
@misc{WeAreSocial2016,
author = {{We Are Social}},
title = {{We Are Social Singapore}},
url = {https://de.slideshare.net/wearesocialsg},
note = { Zugriff: 2017-09-05},
year = {2016}
}
@misc{RegistrarStats2017,
author = {RegistrarStats},
title = {{TLD Domain Counts}},
url = {http://www.registrarstats.com/TLDDomainCounts.aspx},
note = { Zugriff: 2017-09-05},
year = {2017}
}
@book{Eds2013,
author = {Eds, Andreas Ulrich and Wg, Ifip and Conference, International and Hutchison, David},
file = {:D$\backslash$:/Daten/cloud.aborgardt.com/Documents/Studium/Masterstudium/[000] Masterarbeit/Materialien/10.1007{\_}978-3-642-41707-8.pdf:pdf},
isbn = {9783642417061},
number = {November},
title = {{LNCS 8254 - Testing Software and Systems}},
year = {2013}
}
@misc{Diehl2015,
author = {Diehl, Christoph},
keywords = {Context-free grammars,dharma,generator},
title = {{Dharma}},
url = {https://blog.mozilla.org/security/2015/06/29/dharma/},
note = { Zugriff: 2017-08-30},
year = {2015}
}
@article{Wang2009,
abstract = {Wang et al. (Softw. Pract. Exper. 2007; 37(7):727–745) observed a phenomenon of performance inconsistency in the graphics of Java Abstract Window Toolkit (AWT)/Swing among different Java runtime environments (JREs) on Windows XP. This phenomenon makes it difficult to predict the performance of Java game applications. Therefore, they proposed a portable AWT/Swing architecture, called CYC Window Toolkit (CWT), to provide programmers with high and consistent rendering performance for Java game development among different JREs. They implemented a DirectX version to demonstrate the feasibility of the architecture. This paper extends the above research to other environments in two aspects. First, we evaluate the rendering performance of the original Java AWT with different combinations of JREs, image application programming interfaces, system properties and operating systems (OSs), including Windows XP, Windows Vista, Fedora and Mac OS X. The evaluation results indicate that the performance inconsistency of Java AWT also exists among the four OSs, even if the same hardware configuration is used. Second, we design an OpenGL version of CWT, named CWT-GL, to take advantage of modern 3D graphics cards, and compare the rendering performance of CWT with Java AWT/Swing. The results show that CWT-GL achieves more consistent and higher rendering performance in JREs 1.4 to 1.6 on the four OSs. The results also hint at two approaches: (a) decouple the rendering pipelines of Java AWT/Swing from the JREs for faster upgrading and supporting old JREs and (b) use other graphics libraries, such as CWT, instead of Java AWT/Swing to develop cross-platform Java games with higher and more consistent rendering performance. Copyright {\textcopyright} 2009 John Wiley {\&} Sons, Ltd.},
annote = {- nutzt cfg = context free gammars
- tool: gena
- tested on java tools},
archivePrefix = {arXiv},
arxivId = {1008.1900},
author = {Guo, Hai-Feng and Qiu, Zongyan},
doi = {10.1002/spe},
eprint = {1008.1900},
file = {:D$\backslash$:/Daten/cloud.aborgardt.com/Documents/Studium/Masterstudium/[000] Masterarbeit/Materialien/Guo{\_}et{\_}al-2015-Software{\_}{\_}Practice{\_}and{\_}Experience(1).pdf:pdf},
isbn = {0000000000000},
issn = {00380644},
journal = {Software - Practice and Experience},
keywords = {CYC Window Toolkit,Directx,Linux,Mac OS x,OpenGL,Windows},
number = {7},
pages = {701--736},
pmid = {20926156},
title = {{A dynamic stochastic model for automatic grammar-based test generation}},
volume = {39},
year = {2009}
}
@article{Madhavan2015,
annote = {- semantisches pr{\"{u}}fen Kontextfreier Grammatiken
- auffinden von gegenbeispielen, welche gleichheit wiederlegen
- implementierung eines lehrprogramms, welches eingereichte grammatiken von sch{\"{u}}lern verglichen hat},
author = {Madhavan, Ravichandhran and Mayer, Mika\"{e}l and Gulwani, Sumit and Kuncak, Viktor and Madhavan, Ravichandhran and Mayer, Mika\"{e}l and Gulwani, Sumit and Kuncak, Viktor},
doi = {10.1145/2858965.2814304},
file = {:D$\backslash$:/Daten/cloud.aborgardt.com/Documents/Studium/Masterstudium/[000] Masterarbeit/Materialien/Automating-grammar-comparison.pdf:pdf},
isbn = {978-1-4503-3689-5},
issn = {03621340},
journal = {ACM SIGPLAN Notices},
keywords = {Context-free grammars,counter-examples,equivalence,proof system,tutoring system},
number = {10},
pages = {183--200},
title = {{Automating grammar comparison}},
url = {http://dl.acm.org/citation.cfm?doid=2858965.2814304},
volume = {50},
year = {2015}
}
@inproceedings{Heam2011,
abstract = {Random testing represents a simple and tractable way for software assessment. This paper presents the Seed tool that can be used for the uniform random generation of recursive data structures such as labelled trees and logical formulas. We show how Seed can be used in several testing contexts, from model based testing to performance testing. Generated data structures are defined by grammar-like rules, given in an XML format, multiplying Seed possible applications. Seed is based on combinatorial techniques, and can generate uniformly at random k structures of size n with an efficient time complexity. Finally, Seed is available as a free Java application and a great effort has been made to make it easy-to-use.},
annote = {- tool: seed
-- recursive data structure generation
-- xml-datei als beschreibung der grammatik
-- abbilden verschiedener modelle auf xml-b{\"{a}}ume},
author = {H{\'{e}}am, P. C. and Nicaud, C.},
booktitle = {Proceedings - 4th IEEE International Conference on Software Testing, Verification, and Validation, ICST 2011},
doi = {10.1109/ICST.2011.31},
file = {:D$\backslash$:/Daten/cloud.aborgardt.com/Documents/Studium/Masterstudium/[000] Masterarbeit/Materialien/05770595.pdf:pdf},
isbn = {9780769543420},
issn = {2159-4848},
keywords = {Random testing,grammar based testing,tool},
pages = {60--69},
title = {{Seed: An easy-to-use random generator of recursive data structures for testing}},
year = {2011}
}
@article{Artzi2010,
abstract = {Web script crashes and malformed dynamically generated webpages are common errors, and they seriously impact the usability of Web applications. Current tools for webpage validation cannot handle the dynamically generated pages that are ubiquitous on today's Internet. We present a dynamic test generation technique for the domain of dynamic Web applications. The technique utilizes both combined concrete and symbolic execution and explicit-state model checking. The technique generates tests automatically, runs the tests capturing logical constraints on inputs, and minimizes the conditions on the inputs to failing tests so that the resulting bug reports are small and useful in finding and fixing the underlying faults. Our tool Apollo implements the technique for the PHP programming language. Apollo generates test inputs for a Web application, monitors the application for crashes, and validates that the output conforms to the HTML specification. This paper presents Apollo's algorithms and implementation, and an experimental evaluation that revealed 673 faults in six PHP Web applications.},
annote = {- goal: find 2 kind of failures:
-- execution failures
-- HTML failures

- whitebox
- tool: Apollo 
-- creates and executes inputs on web applications
--- modified version of the php interpreter
-- uses html-validators as an oracle
-- tracking of the state (database, session, cookies)
- evaluated on six real web applications},
author = {Artzi, S and Kiezun, A and Dolby, J and Tip, F and Dig, D and Paradkar, A and Ernst, M D},
doi = {10.1109/TSE.2010.31},
file = {:D$\backslash$:/Daten/cloud.aborgardt.com/Documents/Studium/Masterstudium/[000] Masterarbeit/Materialien/05416728.pdf:pdf},
isbn = {0098-5589},
issn = {00985589},
journal = {IEEE Transactions on Software Engineering},
keywords = {PHP,Software testing,Web applications,dynamic analysis,reliability,verification},
number = {4},
pages = {474--494},
title = {{Finding bugs in web applications using dynamic test generation and explicit-state model checking}},
volume = {36},
year = {2010}
}
@article{Viterbi1967,
abstract = {The probability of error in decoding an optimal convolutional code transmitted over a memoryless channel is bounded from above and below as a function of the constraint length of the code. For all but pathological channels the bounds are asymptotically (exponentially) tight for rates above{\textless}tex{\textgreater}R{\_}{\{}0{\}}{\textless}/tex{\textgreater}, the computational cutoff rate of sequential decoding. As a function of constraint length the performance of optimal convolutional codes is shown to be superior to that of block codes of the same length, the relative improvement increasing with rate. The upper bound is obtained for a specific probabilistic nonsequential decoding algorithm which is shown to be asymptotically optimum for rates above{\textless}tex{\textgreater}R{\_}{\{}0{\}}{\textless}/tex{\textgreater}and whose performance bears certain similarities to that of sequential decoding algorithms.},
author = {Viterbi, A.},
doi = {10.1109/TIT.1967.1054010},
file = {:C$\backslash$:/Users/alex/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Viterbi - 1967 - Error bounds for convolutional codes and an asymptotically optimum decoding algorithm.pdf:pdf},
isbn = {0018-9448},
issn = {0018-9448},
journal = {IEEE Transactions on Information Theory},
number = {2},
pages = {260--269},
title = {{Error bounds for convolutional codes and an asymptotically optimum decoding algorithm}},
url = {http://ieeexplore.ieee.org/document/1054010/},
volume = {13},
year = {1967}
}
@inproceedings{Nunan2012,
abstract = {The structure of dynamic websites comprised of a set of objects such as HTML tags, script functions, hyperlinks and advanced features in browsers lead to numerous resources and interactiveness in services currently provided on the Internet. However, these features have also increased security risks and attacks since they allow malicious codes injection or XSS (Cross-Site Scripting). XSS remains at the top of the lists of the greatest threats to web applications in recent years. This paper presents the experimental results obtained on XSS automatic classification in web pages using Machine Learning techniques. We focus on features extracted from web document content and URL. Our results demonstrate that the proposed features lead to highly accurate classification of malicious page.},
author = {Nunan, Angelo Eduardo and Souto, Eduardo and {Dos Santos}, Eulanda M. and Feitosa, Eduardo},
booktitle = {Proceedings - IEEE Symposium on Computers and Communications},
doi = {10.1109/ISCC.2012.6249380},
file = {:C$\backslash$:/Users/alex/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nunan et al. - 2012 - Automatic classification of cross-site scripting in web pages using document-based and URL-based features.pdf:pdf},
isbn = {9781467327121},
issn = {15301346},
keywords = {cross-site scripting,machine learning,scripting languages security,web application security},
pages = {000702--000707},
title = {{Automatic classification of cross-site scripting in web pages using document-based and URL-based features}},
year = {2012}
}
@article{Nguyen2016,
author = {Nguyen, Trong Kha and Hwang, Seong Oun},
doi = {10.1109/CSCI.2016.186},
file = {:C$\backslash$:/Users/alex/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nguyen, Hwang - 2016 - Large-Scale Detection of DOM-based XSS based on Publisher and Subscriber Model.pdf:pdf},
isbn = {9781509055104},
title = {{Large-Scale Detection of DOM-based XSS based on Publisher and Subscriber Model}},
year = {2016}
}
@article{Avancini2012,
abstract = {The goal of security testing is to detect those defects that could be exploited to conduct attacks. Existing works, however, address security testing mostly from the point of view of automatic generation of test cases. Less attention is paid to the problem of developing and integrating with a security oracle. In this paper we address the problem of the security oracle, in particular for Cross-Site Scripting vulnerabilities. We rely on existing test cases to collect HTML pages in safe conditions, i.e. when no attack is run. Pages are then used to construct the safe model of the application under analysis, a model that describes the structure of an application response page for safe input values. The oracle eventually detects a successful attack when a test makes the application display a web page that is not compliant with the safe model.},
author = {Avancini, Andrea and Ceccato, Mariano},
doi = {10.1109/IWAST.2012.6228984},
file = {:C$\backslash$:/Users/alex/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Avancini, Ceccato - 2012 - Grammar based oracle for security testing of web applications.pdf:pdf},
isbn = {9781467318228},
journal = {2012 7th International Workshop on Automation of Software Test, AST 2012 - Proceedings},
keywords = {cross site scripting,security testing,test oracle},
number = {line 13},
pages = {15--21},
title = {{Grammar based oracle for security testing of web applications}},
year = {2012}
}
@book{Connolly2015,
abstract = {At a time of rapid business globalisation, it is necessary to understand employee security behaviour within diverse cultural settings. While general deterrence theory has been extensively used in Behavioural Information Security research with the aim to explain the effect of deterrent factors on employees' security actions, these studies provide inconsistent and even contradictory findings. Therefore, a further examination of deterrent factors in the security context is required. The aim of this study is to contribute to the emerging field of Behavioural Information Security research by investigating how a combination of security countermeasures and cultural factors impact upon employee security behaviour in organisations. A particular focus of this project is to explore the effect of national culture and organisational culture on employee actions as regards information security. Preliminary findings suggest that organisational culture, national culture, and security countermeasures do have an impact upon employee security behaviour.},
archivePrefix = {arXiv},
arxivId = {arXiv:1210.6621v2},
author = {Connolly, Lena and Lang, Michael and Tygar, J. D.},
booktitle = {IFIP Advances in Information and Communication Technology},
doi = {10.1007/978-3-319-18467-8},
eprint = {arXiv:1210.6621v2},
file = {:C$\backslash$:/Users/alex/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Connolly, Lang, Tygar - 2015 - ICT Systems Security and Privacy Protection.pdf:pdf},
isbn = {978-3-319-18466-1},
issn = {18684238},
keywords = {Employee security behavior,National culture,Organisational culture,Security countermeasures},
pages = {283--296},
title = {{ICT Systems Security and Privacy Protection}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84942582480\&partnerID=tZOtx3y1},
volume = {455},
year = {2015}
}
@article{Simos2014,
author = {Simos, Dimitris E},
file = {:C$\backslash$:/Users/alex/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Simos - 2014 - The Mathematics behind an Automated Penetration Testing Framework.pdf:pdf},
keywords = {Freret,Louisiana,New Orleans},
title = {{The Mathematics behind an Automated Penetration Testing Framework}},
url = {https://www.securityforum.at/wp-content/uploads/2014/05/SF14\_Slides\_Simos.pdf},
year = {2014}
}
@article{Mohan,
abstract = {Web applications are one of the most widespread platforms for information and service delivery over the Internet. Because of the widely exposed feature of web application or services, any web security vulnerability will mostly be observed and be exploited by hackers. The Structured Query Language (SQL) Injection and Cross Site Scripting(XSS) are the greatest security risks in the world according to the Open Web Application Security Projects (OWASP) Top 10 Security vulnerabilities. One of the most commonly found attack is SQL injection. Today many researches are done in SQL injection. There exists different tool to detect SQl injection .Also XSS is another commonly found attack .There exist method to detect XSS. In this survey, a detailed survey of different types of SQL Injection and XSS attack detection methods. Finally reach a conclusion to develop a tool to detect both SQLI and XSS.},
author = {Mohan, Swapna M},
file = {:C$\backslash$:/Users/alex/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mohan - Unknown - ANALYZING AND DEVELOPING AN AUTOMATED TOOL FOR DETECTION OF BOTH SQLI AND XSS ATTACKS.pdf:pdf},
title = {{ANALYZING AND DEVELOPING AN AUTOMATED TOOL FOR DETECTION OF BOTH SQLI AND XSS ATTACKS}},
url = {http://data.conferenceworld.in/ICSTM2/P684-691.pdf}
}
@inproceedings{Avancini,
abstract = {Security is a crucial concern, especially for those applications, like web-based programs, that are constantly exposed to potentially malicious environments. Security testing aims at verifying the presence of security related defects. Security tests consist of two major parts, input values to run the application and the decision if the actual output matches the expected output, the latter is known as the “oracle”. In this paper, we present a process to build a security oracle for testing Cross-site scripting vulnerabilities in web applications. In the learning phase, we analyze web pages generated in safe conditions to learn a model of their syntactic structure. Then, in the testing phase, the model is used to classify new test cases either as “safe tests” or as “successful attacks”. This approach has been implemented in a tool, called Circe, and empirically assessed in classifying security test cases for two real world open source web applications.},
author = {Avancini, Andrea and Ceccato, Mariano},
booktitle = {Proceedings - Working Conference on Reverse Engineering, WCRE},
doi = {10.1109/WCRE.2013.6671301},
file = {:C$\backslash$:/Users/alex/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Avancini, Fondazione, Kessler - Unknown - Circe A Grammar-Based Oracle for Testing Cross-Site Scripting in Web Applications.pdf:pdf},
isbn = {9781479929313},
issn = {10951350},
pages = {262--271},
title = {{Circe: A grammar-based oracle for testing Cross-site scripting in web applications}},
url = {http://selab.fbk.eu/ceccato/papers/2013/wcre2013.pdf},
year = {2013}
}
@inproceedings{Guo,
abstract = {—In order to detect the Cross-Site Script (XSS) vulnerabilities in the web applications, this paper proposes a method of XSS vulnerability detection using optimal attack vector repertory. This method generates an attack vector repertory automatically, optimizes the attack vector repertory using an optimization model, and detects XSS vulnerabilities in web applications dynamically. To optimize the attack vector repertory, an optimization model is built in this paper with a machine learning algorithm, reducing the size of the attack vector repertory and improving the efficiency of XSS vulnerability detection. Based on this method, an XSS vulnerability detector is implemented, which is tested on 50 real-world websites. The testing results show that the detector can detect a total of 848 XSS vulnerabilities effectively in 24 websites.},
author = {Guo, Xiaobing and Jin, Shuyuan and Zhang, Yaxing},
booktitle = {Proceedings - 2015 International Conference on Cyber-Enabled Distributed Computing and Knowledge Discovery, CyberC 2015},
doi = {10.1109/CyberC.2015.50},
file = {:C$\backslash$:/Users/alex/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Guo, Jin, Zhang - Unknown - XSS Vulnerability Detection Using Optimized Attack Vector Repertory.pdf:pdf},
isbn = {9781467391993},
keywords = {XSS,attack vector repertory,dynamic analysis,machine learning,web crawler},
pages = {29--36},
title = {{XSS Vulnerability Detection Using Optimized Attack Vector Repertory}},
url = {http://ieeexplore.ieee.org/ielx7/7307709/7307766/07307783.pdf?tp=\&arnumber=7307783\&isnumber=7307766},
year = {2015}
}
@article{Bazzoli2014,
abstract = {Since the first publication of the "OWASP Top 10" (2004), cross-site scripting (XSS) vulnerabilities have always been among the top 5 web application security bugs. Black-box vulnerability scanners are widely used in the industry to reproduce (XSS) attacks automatically. In spite of the technical sophistication and advancement, previous work showed that black-box scanners miss a non-negligible portion of vulnerabilities, and report non-existing, non-exploitable or uninteresting vulnerabilities. Unfortunately, these results hold true even for XSS vulnerabilities, which are relatively simple to trigger if compared, for instance, to logic flaws. Black-box scanners have not been studied in depth on this vertical: knowing precisely how scanners try to detect XSS can provide useful insights to understand their limitations, to design better detection methods. In this paper, we present and discuss the results of a detailed and systematic study on 6 black-box web scanners (both proprietary and open source) that we conducted in coordination with the respective vendors. To this end, we developed an automated tool to (1) extract the payloads used by each scanner, (2) distill the "templates" that have originated each payload, (3) evaluate them according to quality indicators, and (4) perform a cross-scanner analysis. Unlike previous work, our testbed application, which contains a large set of XSS vulnerabilities, including DOM XSS, was gradually retrofitted to accomodate for the payloads that triggered no vulnerabilities. Our analysis reveals a highly fragmented scenario. Scanners exhibit a wide variety of distinct payloads, a non-uniform approach to fuzzing and mutating the payloads, and a very diverse detection effectiveness.},
archivePrefix = {arXiv},
arxivId = {1410.4207},
author = {Bazzoli, Enrico and Criscione, Claudio and Maggi, Federico and Zanero, Stefano},
eprint = {1410.4207},
file = {:C$\backslash$:/Users/alex/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bazzoli et al. - Unknown - XSS Peeker A Systematic Analysis of Cross-site Scripting Vulnerability Scanners.pdf:pdf},
title = {{XSS Peeker: A Systematic Analysis of Cross-site Scripting Vulnerability Scanners}},
url = {https://arxiv.org/pdf/1410.4207.pdf http://arxiv.org/abs/1410.4207},
year = {2014}
}
@inproceedings{Bozic2015,
abstract = {Security testing of web applications remains a major problem of software engineering. In order to reveal vulnerabilities, manual and automatic testing approaches use different strategies for detection of certain kinds of inputs that might lead to a security breach. In this paper we compared a state-of-the-art manual testing tool with an automated one that is based on model-based testing. The first tool requires user input from the tester whereas the second one reduces the necessary amount of manual manipulation. Both approaches depend on the corresponding test case generation technique and its produced inputs are executed against the system under test (SUT). For this case we enhance a novel technique, which combines a combinatorial testing technique for input generation and a model-based technique for test execution. In this work the input parameter modelling is improved by adding constraints to generate more comprehensive and sophisticated testing inputs. The evaluated results indicate that both techniques succeed in detecting security leaks in web applications with different results, depending on the background logic of the testing approach. Last but not least, we claim that attack pattern-based combinatorial testing with constraints can be an alternative method for web application security testing, especially when we compare our method to other test generation techniques like fuzz testing.},
author = {Bozic, Josip and Garn, Bernhard and Kapsalis, Ioannis and Simos, Dimitris and Winkler, Severin and Wotawa, Franz},
booktitle = {Proceedings - 2015 IEEE International Conference on Software Quality, Reliability and Security, QRS 2015},
doi = {10.1109/QRS.2015.38},
file = {:C$\backslash$:/Users/alex/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bozic et al. - 2015 - Attack Pattern-Based Combinatorial Testing with Constraints for Web Security Testing.pdf:pdf},
isbn = {9781467379892},
keywords = {Combinatorial testing,attack patterns,injection attacks,model-based testing,web security testing},
pages = {207--212},
title = {{Attack Pattern-Based Combinatorial Testing with Constraints for Web Security Testing}},
year = {2015}
}
@inproceedings{Wang2010,
abstract = {Web applications suffer from cross-site scripting (XSS) attacks that resulting from incomplete or incorrect input sanitization. Learning the structure of attack vectors could enrich the variety of manifestations in generated XSS attacks. In this study, we focus on generating more threatening XSS attacks for the state-of-the-art detection approaches that can find potential XSS vulnerabilities in Web applications, and propose a mechanism for structural learning of attack vectors with the aim of generating mutated XSS attacks in a fully automatic way. Mutated XSS attack generation depends on the analysis of attack vectors and the structural learningmechanism. For the kernel of the learning mechanism, we use a Hidden Markov model (HMM) as the structure of the attack vector model to capture the implicit manner of the attack vector, and this manner is benefited from the syntax meanings that are labeled by the proposed tokenizingmechanism. Bayes theoremis used to determine the number of hidden states in the model for generalizing the structure model. The paper has the contributions are as following: (1) automatically learn the structure of attack vectors from practical data analysis to modeling a structure model of attack vectors, (2) mimic the manners and the elements of attack vectors to extend the ability of testing tool for identifying XSS vulnerabilities, (3) be helpful to verify the flaws of blacklist sanitization procedures ofWeb applications. We evaluated the proposed mechanism by Burp Intruder with a dataset collected from public XSS archives. The results shows that mutated XSS attack generation can identify potential vulnerabilities.},
annote = {- Werten XSS-Attacken (in URL-Form) aus und bilden so Attack Vector Profile
- Mutieren XSS-Angriffsvektoren mittels eines auf "Viterbi" basierenden Algorithmus},
archivePrefix = {arXiv},
arxivId = {1009.3711},
author = {Wang, Yi-Hsun and Mao, Ching-Hao and Lee, Hahn-Ming},
booktitle = {Electronic Proceedings in Theoretical Computer Science},
doi = {10.4204/EPTCS.35.2},
eprint = {1009.3711},
file = {:C$\backslash$:/Users/alex/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2010 - Structural Learning of Attack Vectors for Generating Mutated XSS Attacks.pdf:pdf},
issn = {2075-2180},
keywords = {()},
pages = {15--26},
title = {{Structural Learning of Attack Vectors for Generating Mutated XSS Attacks}},
url = {https://arxiv.org/pdf/1009.3711.pdf},
volume = {35},
year = {2010}
}
@inproceedings{Bozic,
abstract = {The number of potential security threats rises with the increasing number of web applications, which cause tremendous financial and existential implications for developers and users as well. The biggest challenge for security testing is to specify and implement ways in order to detect potential vulnerabilities of the developed system in a never ending quest against new security threats but also to cover already known ones so that a program is suited against typical attack vectors. For these purposes many approaches have been developed in the area of model-based security testing in order to come up with solutions for real-world application problems. These approaches provide theoretical background as well as practical solutions for certain security issues. In this paper, we partially rely on previous work but focus on the representation of attack patterns using UML state diagrams. We extend previous work in combining the attack pattern models with combinatorial testing in order to provide concrete test input, which is submitted to the system under test. With combinatorial testing we capture different combinations of inputs and thus increasing the likelihood to find weaknesses in the implementation under test that can be exploited. Besides the foundations of our approach we further report on first experiments that indicate its practical use.},
annote = {Attack-Grammar for XSS},
author = {Bozic, J.a and Simos, D.E.b and Wotawa, F.a},
booktitle = {Proceedings of the 9th International Workshop on Automation of Software Test - AST 2014},
doi = {10.1145/2593501.2593502},
file = {:C$\backslash$:/Users/alex/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bozic, Simos, Wotawa - Unknown - Attack Pattern-Based Combinatorial Testing.pdf:pdf},
isbn = {9781450328586},
keywords = {Combinatorics,G2 [Discrete Mathematics],Security Keywords Combinatorial testing,Security and Protection General Terms Theory,Testing and Debugging,model-based testing,security testing},
pages = {1--7},
title = {{Attack pattern-based combinatorial testing}},
year = {2014}
}
@book{Buchmann,
author = {Buchmann, Johannes},
file = {:C$\backslash$:/Users/alex/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Buchmann - Unknown - Einf{\"{u}}hrung in die Kryptographie.pdf:pdf},
isbn = {9783642397745},
title = {{Einf{\"{u}}hrung in die Kryptographie}}
}
@article{Bijjou2015,
author = {Bijjou, Khalil},
file = {:C$\backslash$:/Users/alex/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bijjou - 2015 - Web Application Firewall Bypassing – how to defeat the blue team.pdf:pdf},
title = {{Web Application Firewall Bypassing – how to defeat the blue team}},
year = {2015}
}
@inproceedings{Kals2006,
abstract = {As the popularity of the web increases and web applications become tools of everyday use, the role of web security has been gaining importance as well. The last years have shown a significant increase in the number of web-based attacks. For example, there has been extensive press coverage of recent security incidences involving the loss of sensitive credit card information belonging to millions of customers.Many web application security vulnerabilities result from generic input validation problems. Examples of such vulnerabilities are SQL injection and Cross-Site Scripting (XSS). Although the majority of web vulnerabilities are easy to understand and to avoid, many web developers are, unfortunately, not security-aware. As a result, there exist many web sites on the Internet that are vulnerable.This paper demonstrates how easy it is for attackers to automatically discover and exploit application-level vulnerabilities in a large number of web applications. To this end, we developed SecuBat, a generic and modular web vulnerability scanner that, similar to a port scanner, automatically analyzes web sites with the aim of finding exploitable SQL injection and XSS vulnerabilities. Using SecuBat, we were able to find many potentially vulnerable web sites. To verify the accuracy of SecuBat, we picked one hundred interesting web sites from the potential victim list for further analysis and confirmed exploitable flaws in the identified web pages. Among our victims were well-known global companies and a finance ministry. Of course, we notified the administrators of vulnerable sites about potential security problems. More than fifty responded to request additional information or to report that the security hole was closed.},
author = {Kals, Stefan and Kirda, Engin and Kruegel, Christopher and Jovanovic, Nenad},
booktitle = {WWW '06: Proceedings of the 15th international conference on World Wide Web},
doi = {10.1145/1135777.1135817},
file = {:C$\backslash$:/Users/alex/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kals et al. - 2006 - SecuBat A Web Vulnerability Scanner.pdf:pdf},
isbn = {1595933239},
keywords = {automated vulnerability detection,crawling,cross-site scripting,scanner,security,sql injection,xss},
pages = {247--256},
title = {{SecuBat : A Web Vulnerability Scanner}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.60.4350},
year = {2006}
}
@inproceedings{Wurzinger2009,
abstract = {Due to the increasing amount of Web sites offering features to contribute rich content, and the frequent failure of Web developers to properly sanitize user input, cross-site scripting prevails as the most significant security threat to Web applications. Using cross-site scripting techniques, miscreants can hijack Web sessions, and craft credible phishing sites. Previous work towards protecting against cross-site scripting attacks suffers from various drawbacks, such as practical infeasibility of deployment due to the need for client-side modifications, inability to reliably detect all injected scripts, and complex, error-prone parameterization. In this paper, we introduce SWAP (secure Web application proxy), a server-side solution for detecting and preventing cross-site scripting attacks. SWAP comprises a reverse proxy that intercepts all HTML responses, as well as a modified Web browser which is utilized to detect script content. SWAP can be deployed transparently for the client, and requires only a simple automated transformation of the original Web application. Using SWAP, we were able to correctly detect exploits on several authentic vulnerabilities in popular Web applications.},
author = {Wurzinger, Peter and Platzer, Christian and Ludl, Christian and Kirda, Engin and Kruegel, Christopher},
booktitle = {Proceedings of the 2009 ICSE Workshop on Software Engineering for Secure Systems, SESS 2009},
doi = {10.1109/IWSESS.2009.5068456},
file = {:C$\backslash$:/Users/alex/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wurzinger et al. - 2009 - SWAP Mitigating XSS attacks using a reverse proxy.pdf:pdf},
isbn = {9781424437252},
pages = {33--39},
title = {{SWAP: Mitigating XSS attacks using a reverse proxy}},
year = {2009}
}
@article{Endler2002,
author = {Endler, D},
file = {:C$\backslash$:/Users/alex/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Endler - 2002 - The evolution of cross site scripting attacks.pdf:pdf},
journal = {Whitepaper, iDefense Inc.(May 2002) http://www. {\ldots}},
number = {5},
pages = {1--25},
title = {{The evolution of cross site scripting attacks}},
url = {http://www.leetupload.com/database/Misc/Papers/Asta la Vista/XSS.pdf},
volume = {2},
year = {2002}
}
@article{Vogt2007,
abstract = {Cross-site scripting (XSS) is an attack against web ap- plications in which scripting code is injected into the output of an application that is then sent to a users web browser. In the browser, this scripting code is executed and used to transfer sensitive data to a third party (i.e., the attacker). Currently, most approaches attempt to prevent XSS on the server side by inspecting and modifying the data that is ex- changed between the web application and the user. Un- fortunately, it is often the case that vulnerable applications are not fixed for a considerable amount of time, leaving the users vulnerable to attacks. The solution presented in this paper stops XSS attacks on the client side by tracking the flow of sensitive information inside the web browser. If sen- sitive information is about to be transferred to a third party, the user can decide if this should be permitted or not. As a result, the user has an additional protection layer when surfing the web, without solely depending on the security of the web application.},
author = {Vogt, Philipp and Nentwich, Florian and Jovanovic, Nenad and Kirda, Engin and Kruegel, Christopher and Vigna, Giovanni},
doi = {10.1021/cm801305f},
file = {:C$\backslash$:/Users/alex/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vogt et al. - 2007 - Cross-Site Scripting Prevention with Dynamic Data Tainting and Static Analysis.pdf:pdf},
issn = {08974756},
journal = {Work},
number = {13},
pages = {4188--4190},
title = {{Cross-Site Scripting Prevention with Dynamic Data Tainting and Static Analysis}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.129.2063\&rep=rep1\&type=pdf},
volume = {42},
year = {2007}
}
@article{Heiderich2013,
abstract = {Back in 2007, Hasegawa discovered a novel Cross-Site Scrip- ting (XSS) vector based on the mistreatment of the backtick character in a single browser implementation. This initially looked like an implementation error that could easily be fixed. Instead, as this paper shows, it was the first example of a new class of XSS vectors, the class of mutation-based XSS (mXSS) vectors, which may occur in innerHTML and related properties. mXSS affects all three major browser families: IE, Firefox, and Chrome. We were able to place stored mXSS vectors in high-profile applications like Yahoo! Mail, Rediff Mail, OpenExchange, Zimbra, Roundcube, and several commercial products. m- XSS vectors bypassed widely deployed server-side XSS pro- tection techniques (like HTML Purifier, kses, htmlLawed, Blueprint and Google Caja), client-side filters (XSSAuditor, IE XSS Filter), Web Application Firewall (WAF) systems, as well as Intrusion Detection and Intrusion Prevention Sys- tems (IDS/IPS).We describe a scenario in which seemingly immune entities are being rendered prone to an attack based on the behavior of an involved party, in our case the browser. Moreover, it proves very difficult to mitigate these attacks: In browser implementations, mXSS is closely related to per- formance enhancements applied to the HTML code before rendering; in server side filters, strict filter rules would break many web applications since the mXSS vectors presented in this paper are harmless when sent to the browser. This paper introduces and discusses a set of seven differ- ent subclasses of mXSS attacks, among which only one was previously known. The work evaluates the attack surface, showcases examples of vulnerable high-profile applications, and provides a set of practicable and low-overhead solutions to defend against these kinds of attacks. Permission},
annote = {Sonderfall von XSS-Angriffen f{\"{u}}r RichTextEditoren und WebMailer},
author = {Heiderich, Mario and Schwenk, J},
doi = {10.1145/2508859.2516723},
file = {:C$\backslash$:/Users/alex/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Heiderich, Schwenk - 2013 - mXSS attacks attacking well-secured web-applications by using innerHTML mutations.pdf:pdf},
isbn = {9781450324779},
issn = {15437221},
journal = {Proceedings of the {\ldots}},
keywords = {browser security,cross-site-scripting,innerhtml,mutation-based xss,mxss,web security,xss},
pages = {777--788},
title = {{mXSS attacks: attacking well-secured web-applications by using innerHTML mutations}},
url = {http://dl.acm.org/citation.cfm?id=2516723},
year = {2013}
}
@article{Nguyen-Tuong2005,
abstract = {Most web applications contain security vulnerabilities. The simple and natural ways of creating a web application are prone to SQL injection attacks and cross-site scripting attacks as well as other less common vulnerabilities. In response, many tools have been developed for detecting or mitigating common web application vulnerabilities. Existing techniques either require effort from the site developer or are prone to false positives. This paper presents a fully automated approach to securely hardening web applications. It is based on precisely tracking taintedness of data and checking specifically for dangerous content only in parts of commands and output that came from untrustworthy sources. Unlike previous work in which everything that is derived from tainted input is tainted, our approach precisely tracks taintedness within data values.},
author = {Nguyen-Tuong, Anh and Guarnieri, Salvatore and Greene, Doug and Shirley, Jeff and Evans, David},
doi = {10.1007/0-387-25660-1_20},
file = {:C$\backslash$:/Users/alex/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nguyen-Tuong et al. - 2005 - Automatically hardening web applications using precise tainting.pdf:pdf},
isbn = {038725658X},
issn = {18684238},
journal = {IFIP Advances in Information and Communication Technology},
keywords = {PHP,SQL injection,cross-site scripting attacks,information flow,precise tainting,web security,web vulnerabilities},
number = {December},
pages = {295--307},
title = {{Automatically hardening web applications using precise tainting}},
volume = {181},
year = {2005}
}
@book{Tobergte2013,
abstract = {applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Tobergte, David R. and Curtis, Shirley},
booktitle = {Journal of Chemical Information and Modeling},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/alex/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Stuttard, Pinto - Unknown - No Title.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
keywords = {icle},
number = {9},
pages = {1689--1699},
pmid = {25246403},
title = {{The web application Hacker's Handbook}},
volume = {53},
year = {2013}
}
@article{Ismail2004,
abstract = {Cross-site scripting (XSS) attacks target web sites with Cookie-based session management, resulting in the leakage of privacy information. Although several server-side coun- termeasures for XSS attacks do exist, such techniques have not been applied in a universal manner, because of their deployment overhead and the poor understanding of XSS problems. This paper proposes a client-side system that au- tomatically detects XSS vulnerability by manipulating ei- ther request or server response. The system also shares the indication of vulnerability via a central repository. The pur- pose of the proposed system is twofold: to protect users from XSS attacks, and to warn the web servers with XSS vulner- abilities.},
author = {Ismail, Omar and Etoh, Masashi and Kadobayashi, Youki},
doi = {10.1109/AINA.2004.1283902},
file = {:C$\backslash$:/Users/alex/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ismail, Etoh, Kadobayashi - 2004 - A proposal and implementation of automatic detectioncollection system for cross-site scripting vulner.pdf:pdf},
isbn = {0-7695-2051-0},
journal = {18th International Conference on Advanced Information Networking and Applications, 2004. AINA 2004.},
pages = {145--151},
title = {{A proposal and implementation of automatic detection/collection system for cross-site scripting vulnerability}},
year = {2004}
}
@article{Noguchi2015,
author = {Noguchi, Tadahiro and Washizaki, Hironori},
file = {:C$\backslash$:/Users/alex/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Noguchi, Washizaki - 2015 - History-Based Test Case Prioritization for Black Box Testing using Ant Colony Optimization.pdf:pdf},
isbn = {9781479971251},
keywords = {ant colony,black box testing,test case prioritization},
pages = {2--3},
title = {{History-Based Test Case Prioritization for Black Box Testing using Ant Colony Optimization}},
year = {2015}
}
@article{Luptak2011,
annote = {NonAlpha

http://www.webtoolkitonline.com/javascript-tester.html},
author = {Lupt{\'{a}}k, Pavol},
file = {:C$\backslash$:/Users/alex/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lupt{\'{a}}k - 2011 - Bypassing Web Application Firewalls.pdf:pdf},
keywords = {1 web application firewalls,common problems,csrf,css,ids,implementations,ips,obfuscation,sql injection,waf,xss},
pages = {79--88},
title = {{Bypassing Web Application Firewalls}},
year = {2011}
}
@article{Ahmed2015,
author = {Ahmed, Mazin},
file = {:C$\backslash$:/Users/alex/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ahmed - 2015 - Evading all web-application firewalls xss filters.pdf:pdf},
number = {September},
title = {{Evading all web-application firewalls xss filters}},
year = {2015}
}
@inproceedings{Aydin2014,
abstract = {Web applications need to validate and sanitize user inputs in order to avoid attacks such as Cross Site Scripting (XSS) and SQL Injection. Writing string manipulation code for input validation and sanitization is an error-prone process leading to many vulnerabilities in real-world web applications. Automata-based static string analysis techniques can be used to automatically compute vulnerability signatures (represented as automata) that characterize all the inputs that can exploit a vulnerability. However, there are several factors that limit the applicability of static string analysis techniques in general: 1) undesirability of static string analysis requires the use of approximations leading to false positives, 2) static string analysis tools do not handle all string operations, 3) dynamic nature of the scripting languages makes static analysis difficult. In this paper, we show that vulnerability signatures computed for deliberately insecure web applications (developed for demonstrating different types of vulnerabilities) can be used to generate test cases for other applications. Given a vulnerability signature represented as an automaton, we present algorithms for test case generation based on state, transition, and path coverage. These automatically generated test cases can be used to test applications that are not analyzable statically, and to discover attack strings that demonstrate how the vulnerabilities can be exploited. {\textcopyright} 2014 IEEE.},
author = {Aydin, Abdulbaki and Alkhalaf, Muath and Bultan, Tevfik},
booktitle = {Proceedings - IEEE 7th International Conference on Software Testing, Verification and Validation, ICST 2014},
doi = {10.1109/ICST.2014.32},
file = {:C$\backslash$:/Users/alex/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Aydin, Alkhalaf, Bultan - 2014 - Automated test generation from vulnerability signatures.pdf:pdf},
isbn = {9780769551852},
issn = {2159-4848},
keywords = {automata-based test generation,string analysis,validation and sanitization,vulnerability signatures},
pages = {193--202},
title = {{Automated test generation from vulnerability signatures}},
year = {2014}
}
@inproceedings{Wassermann2008,
abstract = {Web applications routinely handle sensitive data, and many people rely on them to support various daily activities, so errors can have severe and broad-reaching consequences. Unlike most desktop applications, many web applications are written in scripting languages, such as PHP. The dynamic features commonly supported by these languages significantly inhibit static analysis and existing static analysis of these languages can fail to produce meaningful results on real-world web applications. Automated test input generation using the concolic testing framework has proven useful for finding bugs and improving test coverage on C and Java programs, which generally emphasize numeric values and pointer-based data structures. However, scripting languages, such as PHP, promote a style of programming for developing web applications that emphasizes string values, objects, and arrays. In this paper, we propose an automated input test generation algorithm that uses runtime values to analyze dynamic code, models the semantics of string operations, and handles operations whose argument and return values may not share a common type. As in the standard concolic testing framework, our algorithm gathers constraints during symbolic execution. Our algorithm resolves constraints over multiple types by considering each variable instance individually, so that it only needs to invert each operation. By recording.},
author = {Wassermann, Gary and Yu, Dachuan and Chander, Ajay and Dhurjati, Dinakar and Inamura, Hiroshi and Su, Zhendong and {Wassermann G.}, Yu D Chander a Dhurjati D Inamura H Su Z},
booktitle = {Proceedings of the 2008 international symposium on Software testing and analysis},
doi = {10.1145/1390630.1390661},
file = {:C$\backslash$:/Users/alex/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wassermann et al. - 2008 - Dynamic test input generation for web applications.pdf:pdf},
isbn = {9781605580500},
keywords = {Applications,Automated tests,Computer softwar,Computer software,Concol,Software testing,automatic test generation,concolic testing,directed random testing,web applications},
pages = {249--260},
title = {{Dynamic test input generation for web applications}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-57449103850\&partnerID=40\&md5=568a7526d17f15677c2a3bbecdaa8db3\%5Cnhttp://doi.acm.org/10.1145/1390630.1390661},
year = {2008}
}
@article{Qu2007,
abstract = {Test case prioritization is an effective and practical technique that helps to increase the rate of regression fault detection when software evolves. Numerous techniques have been reported in the literature on prioritizing test cases for regression testing. However, existing prioritization techniques implicitly assume that source or binary code is available when regression testing is performed, and therefore cannot be implemented when there is no program source or binary code to be analyzed. In this paper, we presented a new technique for black box regression testing, and we performed an experiment to measure our technique. Our results show that the new technique is helpful to improve the effectiveness of fault detection when performing regression test in black box environment. {\textcopyright} 2007 IEEE.},
author = {Qu, Bo and Nie, Changhai and Xu, Baowen and Zhang, Xiaofang},
doi = {10.1109/COMPSAC.2007.209},
file = {:C$\backslash$:/Users/alex/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Qu et al. - 2007 - Test Case Prioritization for Black Box Testing.pdf:pdf},
isbn = {0-7695-2870-8},
issn = {0730-3157},
journal = {31st Annual International Computer Software and Applications Conference - Vol. 1- (COMPSAC 2007)},
keywords = {black box testing,prioritization,regression testing,test history},
number = {1},
pages = {465--474},
title = {{Test Case Prioritization for Black Box Testing}},
url = {http://ieeexplore.ieee.org/document/4291039/},
year = {2007}
}
@article{Mohammed2014,
author = {Mohammed, Mohammed Ali},
file = {:C$\backslash$:/Users/alex/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mohammed - 2014 - Master ' S Thesis.pdf:pdf},
number = {September},
title = {{Master ' S Thesis}},
year = {2014}
}
@article{Doupe2012,
abstract = {Black-box web vulnerability scanners are a popular choice for finding security vulnerabilities in web applications in an automated fashion. These tools operate in a point-and-shootmanner, testing any web application-- regardless of the server-side language--for common security vulnerabilities. Unfortunately, black-box tools suffer from a number of limitations, particularly when interacting with complex applications that have multiple actions that can change the application's state. If a vulnerability analysis tool does not take into account changes in the web application's state, it might overlook vulnerabilities or completely miss entire portions of the web application. We propose a novel way of inferring the web application's internal state machine from the outside--that is, by navigating through the web application, observing differences in output, and incrementally producing a model representing the web application's state. We utilize the inferred state machine to drive a black-box web application vulnerability scanner. Our scanner traverses a web application's state machine to find and fuzz user-input vectors and discover security flaws. We implemented our technique in a prototype crawler and linked it to the fuzzing component from an open-source web vulnerability scanner. We show that our state-aware black-box web vulnerability scanner is able to not only exercise more code of the web application, but also discover vulnerabilities that other vulnerability scanners miss.},
author = {Doup{\'{e}}, Adam and Cavedon, Ludovico and Kruegel, Christopher and Vigna, Giovanni},
file = {:C$\backslash$:/Users/alex/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Doup{\'{e}} et al. - 2012 - Enemy of the State A State-Aware Black-Box Web Vulnerability Scanner.pdf:pdf},
isbn = {978-931971-95-9},
journal = {USENIX Security Symposium},
pages = {523--538},
title = {{Enemy of the State: A State-Aware Black-Box Web Vulnerability Scanner}},
url = {https://www.usenix.org/conference/usenixsecurity12/technical-sessions/presentation/doupe},
year = {2012}
}
@article{Holler2012,
abstract = {Fuzz testing is an automated technique providing random data as input to a software system in the hope to expose a vulnerability. In order to be effective, the fuzzed input must be common enough to pass elementary consistency checks; a JavaScript interpreter, for instance, would only accept a semantically valid program. On the other hand, the fuzzed input must be uncommon enough to trigger exceptional behavior, such as a crash of the interpreter. The LangFuzz approach resolves this conflict by using a grammar to randomly generate valid programs; the code fragments, however, partially stem from programs known to have caused invalid behavior before. LangFuzz is an effective tool for security testing: Applied on the Mozilla JavaScript interpreter, it discovered a total of 105 new severe vulnerabilities within three months of operation (and thus became one of the top security bug bounty collectors within this period); applied on the PHP interpreter, it discovered 18 new defects causing crashes.},
author = {Holler, Christian and Herzig, Kim and Zeller, Andreas},
file = {:C$\backslash$:/Users/alex/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Holler, Herzig, Zeller - 2012 - Fuzzing with Code Fragments.pdf:pdf},
isbn = {978-931971-95-9},
journal = {Usenix},
keywords = {security, security testing, fuzz testing, grammar},
pages = {38},
title = {{Fuzzing with Code Fragments}},
url = {http://dl.acm.org/citation.cfm?id=2362793.2362831},
year = {2012}
}
@inproceedings{Emmi2007,
abstract = {We describe an algorithm for automatic test input generation for database applications. Given a program in an imperative language that interacts with a database through API calls, our algorithm generates both input data for the program as well as suitable database records to systematically explore all paths of the program, including those paths whose execution depend on data returned by database queries. Our algorithm is based on concolic execution, where the program is run with concrete inputs and simultaneously also with symbolic inputs for both program variables as well as the database state. The symbolic constraints generated along a path enable us to derive new input values and new database records that can cause execution to hit uncovered paths. Simultaneously, the concrete execution helps to retain precision in the symbolic computations by allowing dynamic values to be used in the symbolic executor. This allows our algorithm, for example, to identify concrete SQL queries made by the program, even if these queries are built dynamically. The contributions of this paper are the following. We develop an algorithm that can track symbolic constraints across language boundaries and use those constraints in conjunction with a novel constraint solver to generate both program inputs and database state. We propose a constraint solver that can solve symbolic constraints consisting of both linear arithmetic constraints over variables as well as string constraints (string equality, disequality, as well as membership in regular languages). Finally, we provide an evaluation of the algorithm on a Java implementation of MediaWiki, a popular wiki package that interacts with a database back-end.},
author = {Emmi, Michael and Berkeley, U C},
booktitle = {Proceedings of the 2007 international symposium on Software testing and analysis},
doi = {10.1145/1273463.1273484},
file = {:C$\backslash$:/Users/alex/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Emmi, Berkeley - 2007 - Dynamic test input generation for database applications.pdf:pdf},
title = {{Dynamic test input generation for database applications}},
url = {http://www.mendeley.com/research/dynamic-test-input-generation-database-applications},
year = {2007}
}
@inproceedings{Pham2016,
abstract = {Many real-world programs take highly structured and complex files as inputs. The automated testing of such programs is non-trivial. If the test does not adhere to a specific file format, the program returns a parser error. For symbolic execution-based whitebox fuzzing the corresponding error handling code becomes a significant time sink. Too much time is spent in the parser exploring too many paths lead-ing to trivial parser errors. Naturally, the time is better spent explor-ing the functional part of the program where failure with valid input exposes deep and real bugs in the program. In this paper, we suggest to leverage information about the file format and data chunks of existing, valid files to swiftly carry the exploration beyond the parser code. We call our approach Model-based Whitebox Fuzzing (MoWF) because the file format input model of blackbox fuzzers can be exploited as a constraint on the vast input space to rule out most invalid inputs during path explo-ration in symbolic execution. We evaluate on 13 vulnerabilities in 8 large program binaries with 6 separate file formats and found that MoWF exposes all vulnerabilities while both, traditional whitebox fuzzing and model-based blackbox fuzzing, expose only less than half, respectively. Our experiments also demonstrate that MoWF exposes 70{\%} vulnerabilities without any seed inputs.},
author = {Pham, Van-Thuan and B{\"{o}}hme, Marcel and Roychoudhury, Abhik},
booktitle = {Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
doi = {10.1145/2970276.2970316},
file = {:C$\backslash$:/Users/alex/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pham, B{\"{o}}hme, Roychoudhury - 2016 - Model-Based Whitebox Fuzzing for Program Binaries.pdf:pdf},
title = {{Model-Based Whitebox Fuzzing for Program Binaries}},
url = {http://www.mendeley.com/research/modelbased-whitebox-fuzzing-program-binaries},
year = {2016}
}
@inproceedings{Liang2013,
abstract = {In this paper we present a new vulnerability-targeted black box fuzzing approach to effectively detect errors in the program. Unlike the standard fuzzing techniques that randomly change bytes of the input file, our approach remarkably reduces the fuzzing range by utilizing an efficient dynamic taint analysis technique. It locates the regions of seed files that affect the values used at the hazardous points. Thus it enables to pay more attention to deep errors in the core of the program. Because our approach is directly targeted to the specific potential vulnerabilities, most of the detected errors are with vulnerability signatures. Besides, this approach does not need the information of the input file format in advance. So it is especially appropriate for testing applications with complex and highly structured input file formats. We design and implement a prototype, Taint Fuzz, to realize this approach. The experiments demonstrate that Taint Fuzz can effectively expose more errors with much lower time cost and much smaller number of input samples compared with the standard fuzzer.},
author = {Liang, Guangcheng and Liao, Lejian and Xu, Xin and Du, Jianguang and Li, Guoqiang and Zhao, Henglong},
booktitle = {Proceedings - 9th International Conference on Computational Intelligence and Security, CIS 2013},
doi = {10.1109/CIS.2013.135},
file = {:C$\backslash$:/Users/alex/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liang et al. - 2013 - Effective fuzzing based on dynamic taint analysis.pdf:pdf},
title = {{Effective fuzzing based on dynamic taint analysis}},
url = {http://www.mendeley.com/research/effective-fuzzing-based-dynamic-taint-analysis},
year = {2013}
}
@inproceedings{Bau2010,
abstract = {Black-box web application vulnerability scanners are automated tools that probe web applications for security vulnerabilities. In order to assess the current state of the art, we obtained access to eight leading tools and carried out a study of: (i) the class of vulnerabilities tested by these scanners, (ii) their effectiveness against target vulnerabilities, and (iii) the relevance of the target vulnerabilities to vulnerabilities found in the wild. To conduct our study we used a custom web application vulnerable to known and projected vulnerabilities, and previous versions of widely used web applications containing known vulnerabilities. Our results show the promise and effectiveness of automated tools, as a group, and also some limitations. In particular, "stored" forms of Cross Site Scripting (XSS) and SQL Injection (SQLI) vulnerabilities are not currently found by many tools. Because our goal is to assess the potential of future research, not to evaluate specific vendors, we do not report comparative data or make any recommendations about purchase of specific tools.},
author = {Bau, Jason and Bursztein, Elie and Gupta, Divij and Mitchell, John},
booktitle = {Proceedings - IEEE Symposium on Security and Privacy},
doi = {10.1109/SP.2010.27},
file = {:C$\backslash$:/Users/alex/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bau et al. - 2010 - State of the art Automated black-box web application vulnerability testing.pdf:pdf},
title = {{State of the art: Automated black-box web application vulnerability testing}},
url = {http://www.mendeley.com/research/state-art-automated-blackbox-web-application-vulnerability-testing},
year = {2010}
}
@article{Salas2015,
abstract = {Web services work over dynamic connections among distributed systems. This technology was specifically designed to easily pass SOAP message through firewalls using open ports. These benefits involve a number of security challenges, such as Injection Attacks, phishing, Denial-of-Services (DoS) attacks, and so on. The difficulty to detect vulnerabilities,before they are exploited, encourages developers to use security testing like penetration testing to reduce the potential attacks. Given a black-box approach, this research use the penetration testing to emulate a series of attacks, such as Cross-site Scripting (XSS), Fuzzing Scan, Invalid Types, Malformed XML, SQL Injection, XPath Injection and XML Bomb. In this way, was used the soapUI vulnerability scanner in order to emulate these attacks and insert malicious scripts in the requests of the web services tested. Furthermore, was developed a set of rules to analyze the responses in order to reduce false positives and negatives. The results suggest that 97.1{\%} of web services have at least one vulnerability of these attacks. We also determined a ranking of these attacks against web services.},
author = {Salas, Marcelo Invert Palma and Martins, Eliane},
doi = {10.1109/TLA.2015.7069095},
file = {:C$\backslash$:/Users/alex/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Salas, Martins - 2015 - A Black-Box Approach to Detect Vulnerabilities in Web Services Using Penetration Testing.pdf:pdf},
journal = {IEEE Latin America Transactions},
title = {{A Black-Box Approach to Detect Vulnerabilities in Web Services Using Penetration Testing}},
url = {http://www.mendeley.com/research/blackbox-approach-detect-vulnerabilities-web-services-using-penetration-testing},
year = {2015}
}
@inproceedings{Duchene2013,
abstract = {Fuzz testing consists of automatically generating and sending malicious inputs to an application in order to hopefully trigger a vulnerability. In order to be efficient, the fuzzing should answer questions such as: Where to send a malicious value? Where to observe its effects? How to position the system in such states? Answering such questions is a matter of understanding precisely enough the application. Reverseengineering is a possible way to gain this knowledge, especially in a black-box harness. In fact, given the complexity of modern web applications, automated black-box scanners alternatively reverse-engineer and fuzz web applications to detect vulnerabilities. We present an approach, named as LigRE, which improves the reverse engineering to guide the fuzzing. We adapt a method to automatically learn a control flow model of web applications, and annotate this model with inferred data flows. Afterwards, we generate slices of the model for guiding the scope of a fuzzer. Empirical experiments show that LigRE increases detection capabilities of Cross Site Scripting (XSS), a particular case of web command injection vulnerabilities.},
annote = {- reverse-engineering approach which guides the fuzzing
- using selenium library controlling the browser
- includes control flow, data flow, slicing and fuzzing},
author = {Duchene, Fabien and Rawat, Sanjay and Richier, Jean Luc and Groz, Roland},
booktitle = {Proceedings - Working Conference on Reverse Engineering, WCRE},
doi = {10.1109/WCRE.2013.6671300},
file = {:C$\backslash$:/Users/alex/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Duchene et al. - 2013 - LigRE Reverse-engineering of control and data flow models for black-box XSS detection.pdf:pdf},
isbn = {9781479929313},
issn = {10951350},
keywords = {Control Flow Inference,Data-Flow Inference,Penetration Testing,Reverse-Engineering,Web Application,XSS},
pages = {252--261},
title = {{LigRE: Reverse-engineering of control and data flow models for black-box XSS detection}},
url = {https://goo.gl/5kpLNL},
year = {2013}
}
@inproceedings{Duchene2014,
abstract = {We present a black-box based smart fuzzing approach to detect cross-site scripting (XSS) vulnerabilities in web applications. The smartness is attributed to model inference and automated malicious input generation. The former is implemented as a state-aware crawler which models the application as an automaton. The second component, evolutionary fuzzing, uses genetic algorithm (GA). To limit its search space, we introduce an Attack Grammar that mimics attackers by generating malicious inputs. XSS are characterized by a relationship between tainted input and output of the web application. In black-box testing, precisely capturing this relationship is a non-trivial task that we solve using taint inference techniques. By doing so, we focus only on the transitions wherein this relationship holds and thereby avoid to fuzz the whole system. GA automates the process of generating inputs by considering the states of the application and evolving the inputs to trigger XSS, if any. Empirical evaluation shows that our fuzzer detects vulnerabilities missed by other blackbox scanners.},
annote = {- evolutionary fuzzing
- attack grammar based
- fitness function
- generic generation of javascript parts
- slicing and keeping successfull attack parts, then fuzzing the rest},
author = {Duchene, Fabien and Rawat, Sanjay and Richier, Jean-Luc and Groz, Roland},
booktitle = {Proceedings of the 4th ACM Conference on Data and Application Security and Privacy},
file = {:C$\backslash$:/Users/alex/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Duchene et al. - 2014 - KameleonFuzz Evolutionary Fuzzing for Black-box XSS Detection.pdf:pdf},
isbn = {978-1-4503-2278-2},
keywords = {black-box security testing,cross-site scripting,evolutionary algorithm,fuzzing,model inference,taint inference},
pages = {37--48},
title = {{KameleonFuzz: Evolutionary Fuzzing for Black-box XSS Detection}},
volume = {1},
url = {http://dx.doi.org/10.1145/2557547.2557550}
year = {2014}
}
@article{Wies2014,
annote = {- evolutionary fuzzing -{\textgreater} kameleon fuzz
- attack grammar based
- random picks of "genes"
- fitness function
- fitnes is increased for every char in a list (see comment)
- generic generation of javascript parts},
author = {Wies, Noam and Zeltser, Benny},
file = {:C$\backslash$:/Users/alex/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wies, Zeltser - 2014 - AI Final Project - Evolutionary XSS Detector.pdf:pdf},
keywords = {()},
number = {2},
pages = {3--7},
title = {{AI Final Project - Evolutionary XSS Detector}},
year = {2014}
}
@inproceedings{Bagheri2012,
author = {Bagheri, E and Ensan, F and Gasevic, D},
booktitle = {Proceedings of the 2012 Conference of {\ldots}},
file = {:C$\backslash$:/Users/alex/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bagheri, Ensan, Gasevic - 2012 - Grammar-based test generation for software product line feature models.pdf:pdf},
title = {{Grammar-based test generation for software product line feature models}},
url = {http://dl.acm.org/citation.cfm?id=2399785},
year = {2012}
}
@phdthesis{Holler2011,
abstract = {Fuzz-Testing (Robustness Testing) is a popular automated testing method to locate defects within software that has proven to be valuable especially in the area of security testing. Several frameworks for typical applications (e.g. network protocols or media files) have been written so far. However, when it comes to interpreter testing, only a few language specific fuzzers exist. In this thesis we will introduce a new fuzzing tool called ”LangFuzz” which provides a language-independent approach for interpreter testing by combining random code generation and code mutation techniques based on syntax. For the languages JavaScript and PHP, we will evaluate the tool and show that it is able to detect real-world defects in the popular browsers Firefox and Chrome. So far, LangFuzz has been awarded with twelve Mozilla Security Bug Bounty Awards and twelve Chromium Security Rewards.},
author = {Holler, Christian},
booktitle = {Master's Thesis Dissertation, Department of Computer {\ldots}},
file = {:C$\backslash$:/Users/alex/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Holler - 2011 - Grammar-Based Interpreter Fuzz Testing.pdf:pdf},
title = {{Grammar-Based Interpreter Fuzz Testing}},
url = {https://users.own-hero.net/~decoder/holler-mthesis-2011.pdf},
year = {2011}
}
@article{Majumdar2007,
abstract = {We present lazy expansion, a new algorithm for scalable test input generation using directed concolic execution. Lazy expansion is an instantiation of the counterexample-guided refinement paradigm from static software verification in the context of testing. Our algorithm works in two phases. It first explores, using concolic execution, an abstraction of the function under test by replacing each called function with an unconstrained input. Second, for each (possibly spurious) trace generated by this abstraction, it attempts to expand the trace to a concretely realizable execution by recursively expanding the called functions and finding concrete executions in the called functions that can be stitched together with the original trace to form a complete program execution. Thus, it reduces the burden of symbolic reasoning about interprocedural paths to reasoning about intraprocedural paths (in the exploration phase), together with a localized and constrained search through functions (in the concretization phase). Lazy expansion is particularly effective in testing functions that make more-or-less independent calls to lower level library functions (that have already been unit tested), by only exploring relevant paths in the function under test. We have implemented our algorithm on top of the CUTE concolic execution tool for C and applied it to testing parser code in small compilers. In preliminary experiments, our tool, called LATEST, outperformed CUTE by an order of magnitude in terms of the time taken to generate inputs, and in contrast to CUTE, produced many syntactically valid input strings which exercised interesting paths through the compiler (rather than only the parser error handling code).},
author = {Majumdar, R and Sen, K},
file = {:C$\backslash$:/Users/alex/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Majumdar, Sen - 2007 - Latest Lazy dynamic test input generation.pdf:pdf},
isbn = {UCB/EECS-2007-36},
journal = {{\ldots} , Tech. Rep. UCB/EECS-2007-36},
title = {{Latest: Lazy dynamic test input generation}},
url = {http://digitalassets.lib.berkeley.edu/techreports/ucb/text/EECS-2007-36.pdf},
year = {2007}
}
@inproceedings{Sato2007,
abstract = {As deep submicron technologies are advanced, new challenges, such as power consumption and soft errors, are emerging. A naive technique, which utilizes emerging multicore processors and relies upon thread-level redundancy to detect soft errors, is power hungry. Another technique, which relies upon instruction-level redundancy, diminishes computing performance seriously. This paper investigates trade-off between power and performance of a dependable multicore processor, which is named multiple clustered core processor (MCCP). It is proposed to hybrid thread- and instruction-level redundancy to achieve both large power efficiency and small performance loss. Detailed simulations show that the MCCP exploiting the hybrid technique improves power efficiency in energy-delay product by 13{\%} when it is compared with the one exploiting the naive thread-level technique.},
annote = {- benchmarking automatic web vulnerability scanners},
author = {Sato, Toshinori and Funaki, Toshimasa},
booktitle = {Proceedings - 13th Pacific Rim International Symposium on Dependable Computing, PRDC 2007},
doi = {10.1109/PRDC.2007.55},
file = {:C$\backslash$:/Users/alex/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sato, Funaki - 2007 - Testing and comparing web vulnerability scanning tools for SQL injection and XSS attacks.pdf:pdf},
isbn = {0769530540},
pages = {268--271},
title = {{Testing and comparing web vulnerability scanning tools for SQL injection and XSS attacks}},
year = {2007}
}
@article{Kie2009,
annote = {- white box testing
- automated tool for finding security vulnerabilities},
author = {Kie, Adam and Guo, Philip J and Ernst, Michael D},
file = {:C$\backslash$:/Users/alex/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kie, Guo, Ernst - 2009 - Automatic Creation of SQL Injection and Cross-Site Scripting Attacks.pdf:pdf},
isbn = {9781424434527},
pages = {199--209},
title = {{Automatic Creation of SQL Injection and Cross-Site Scripting Attacks}},
year = {2009}
}
@article{Cadar2006,
abstract = {This paper presents EXE, an effective bug-finding tool that automatically generates inputs that crash real code. Instead of running code on manually or randomly constructed input, EXE runs it on symbolic input initially allowed to be "anything." As checked code runs, EXE tracks the constraints on each symbolic (i.e., input-derived) memory location. If a statement uses a symbolic value, EXE does not run it, but instead adds it as an input-constraint; all other statements run as usual. If code conditionally checks a symbolic expression, EXE forks execution, constraining the expression to be true on the true branch and false on the other. Because EXE reasons about all possible values on a path, it has much more power than a traditional runtime tool: (1) it can force execution down any feasible program path and (2) at dangerous operations (e.g., a pointer dereference), it detects if the current path constraints allow any value that causes a bug.When a path terminates or hits a bug, EXE automatically generates a test case by solving the current path constraints to find concrete values using its own co-designed constraint solver, STP. Because EXE's constraints have no approximations, feeding this concrete input to an uninstrumented version of the checked code will cause it to follow the same path and hit the same bug (assuming deterministic code).EXE works well on real code, finding bugs along with inputs that trigger them in: the BSD and Linux packet filter implementations, the udhcpd DHCP server, the pcre regular expression library, and three Linux file systems.},
author = {Cadar, Cristian and Ganesh, Vijay and Pawlowski, Peter M and Dill, David L and Engler, Dawson R},
doi = {10.1145/1180405.1180445},
file = {:C$\backslash$:/Users/alex/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cadar et al. - 2006 - EXE Automatically Generating Inputs of Death.pdf:pdf},
isbn = {1595935185},
issn = {10949224},
journal = {Computer},
keywords = {attack generation,bolic execution,bug finding,constraint solving,dynamic analysis,sym,test case generation},
number = {2},
pages = {322--335},
title = {{EXE : Automatically Generating Inputs of Death}},
url = {http://portal.acm.org/citation.cfm?id=1455518.1455522},
volume = {12},
year = {2006}
}
@article{Hydara2015a,
abstract = {Context: Cross-site scripting (XSS) is a security vulnerability that affects web applications. It occurs due to improper or lack of sanitization of user inputs. The security vulnerability caused many problems for users and server applications. Objective: To conduct a systematic literature review on the studies done on XSS vulnerabilities and attacks. Method: We followed the standard guidelines for systematic literature review as documented by Barbara Kitchenham and reviewed a total of 115 studies related to cross-site scripting from various journals and conference proceedings. Results: Research on XSS is still very active with publications across many conference proceedings and journals. Attack prevention and vulnerability detection are the areas focused on by most of the studies. Dynamic analysis techniques form the majority among the solutions proposed by the various studies. The type of XSS addressed the most is reflected XSS. Conclusion: XSS still remains a big problem for web applications, despite the bulk of solutions provided so far. There is no single solution that can effectively mitigate XSS attacks. More research is needed in the area of vulnerability removal from the source code of the applications before deployment.},
annote = {- 2000 - 2012
- 115 Studies
-- 69 (60{\%}) Studies focused on xss attacks
-- 37 (32.2{\%}) Studies focused on xss vulnerabilities
-- 9 (7.8{\%}) combine more than one area
--- 32 (27.8{\%}) studies focused on detecting xss vulnerabilities in running web application},
author = {Hydara, Isatou and Sultan, Abu Bakar Md and Zulzalil, Hazura and Admodisastro, Novia},
doi = {10.1016/j.infsof.2014.07.010},
file = {:C$\backslash$:/Users/alex/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hydara et al. - 2015 - Current state of research on cross-site scripting (XSS) - A systematic literature review(3).pdf:pdf},
issn = {09505849},
journal = {Information and Software Technology},
keywords = {Cross-site scripting,Security,Systematic literature review,Web applications},
number = {July},
pages = {170--186},
publisher = {Elsevier B.V.},
title = {{Current state of research on cross-site scripting (XSS) - A systematic literature review}},
url = {http://dx.doi.org/10.1016/j.infsof.2014.07.010},
volume = {58},
year = {2015}
}
